{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e43c6",
   "metadata": {},
   "source": [
    "âœ… 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28262178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# AIF360 (Fairness toolkit)\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbf1ae",
   "metadata": {},
   "source": [
    "âœ… 2. Simulated Biased Dataset (Mini HR Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ExperienceYears': [5, 2, 7, 6, 8, 2, 4, 1, 6, 3, 2, 5, 7, 1, 8, 4],\n",
    "    'EducationLevel':  [2, 3, 2, 4, 3, 1, 2, 2, 4, 2, 3, 1, 2, 1, 4, 3],\n",
    "    'PerformanceScore':[3, 4, 3, 5, 4, 2, 2, 1, 4, 3, 3, 1, 3, 1, 4, 3],\n",
    "    'Gender':           ['Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female',\n",
    "                         'Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female'],\n",
    "    'HispanicLatino':   ['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No',\n",
    "                         'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes'],\n",
    "    'Termd':            [1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1]  # 1=Terminated, 0=Stayed\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962507a",
   "metadata": {},
   "source": [
    "âœ… 3. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a10fa237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df.copy()\n",
    "df_encoded['Gender'] = LabelEncoder().fit_transform(df_encoded['Gender'])  # Male=1, Female=0\n",
    "df_encoded['HispanicLatino'] = LabelEncoder().fit_transform(df_encoded['HispanicLatino'])  # Yes=1, No=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7540a2",
   "metadata": {},
   "source": [
    "# ðŸ”¨ Step 1: Train the Unfair Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66085a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfair Baseline Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57         2\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.20      0.50      0.29         5\n",
      "weighted avg       0.16      0.40      0.23         5\n",
      "\n",
      "AUC Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = df_encoded.drop(columns='Termd')\n",
    "y = df_encoded['Termd']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train unfair logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and labels\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Unfair Baseline Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf44ef",
   "metadata": {},
   "source": [
    "# ðŸ“‰ Step 2: Detect Bias with Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13c769",
   "metadata": {},
   "source": [
    "(A) Custom Bias Metrics (Group Means & AUCs by Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bias by Gender ---\n",
      "Gender\n",
      "Female    0.398251\n",
      "Male      0.343228\n",
      "Name: predicted_prob, dtype: float64\n",
      "AUC for Female: 1.000\n",
      "AUC for Male: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_df = X_test.copy()\n",
    "results_df['true_label'] = y_test.values\n",
    "results_df['predicted_prob'] = y_prob\n",
    "\n",
    "# Add back readable labels\n",
    "results_df['Gender'] = df.loc[X_test.index, 'Gender'].values\n",
    "results_df['HispanicLatino'] = df.loc[X_test.index, 'HispanicLatino'].values\n",
    "\n",
    "# Bias by Gender: mean predicted probability + AUC per group\n",
    "print(\"\\n--- Bias by Gender ---\")\n",
    "print(results_df.groupby('Gender')['predicted_prob'].mean())\n",
    "\n",
    "for gender in results_df['Gender'].unique():\n",
    "    subset = results_df[results_df['Gender'] == gender]\n",
    "    try:\n",
    "        auc = roc_auc_score(subset['true_label'], subset['predicted_prob'])\n",
    "    except:\n",
    "        auc = float('nan')\n",
    "    print(f\"AUC for {gender}: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1e9ff",
   "metadata": {},
   "source": [
    "(B) AIF360 Bias Metrics (Statistical Parity Difference, Disparate Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e9c47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AIF360 Fairness Metrics by Gender ---\n",
      "Statistical Parity Difference: 0.75\n",
      "Disparate Impact: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\aif360\\metrics\\dataset_metric.py:82: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Repackage test set for AIF360\n",
    "dataset_test = BinaryLabelDataset(\n",
    "    df=pd.concat([X_test, y_test], axis=1),\n",
    "    label_names=['Termd'],\n",
    "    protected_attribute_names=['Gender']\n",
    ")\n",
    "\n",
    "# Add predictions as scores to test set\n",
    "dataset_test_pred = dataset_test.copy()\n",
    "dataset_test_pred.scores = y_prob.reshape(-1, 1)\n",
    "\n",
    "# AIF360 Fairness Metrics\n",
    "metric = BinaryLabelDatasetMetric(\n",
    "    dataset_test_pred,\n",
    "    privileged_groups=[{'Gender': 1}],  # Male = 1\n",
    "    unprivileged_groups=[{'Gender': 0}]  # Female = 0\n",
    ")\n",
    "\n",
    "print(\"\\n--- AIF360 Fairness Metrics by Gender ---\")\n",
    "print(\"Statistical Parity Difference:\", metric.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric.disparate_impact())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
