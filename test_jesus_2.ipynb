{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e43c6",
   "metadata": {},
   "source": [
    "âœ… 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28262178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\inFairness\\utils\\ndcg.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  vect_normalized_discounted_cumulative_gain = vmap(\n",
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\inFairness\\utils\\ndcg.py:48: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.vmap` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.vmap` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  monte_carlo_vect_ndcg = vmap(vect_normalized_discounted_cumulative_gain, in_dims=(0,))\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# AIF360 (Fairness toolkit)\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbf1ae",
   "metadata": {},
   "source": [
    "âœ… 2. Simulated Biased Dataset (Mini HR Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec85a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ExperienceYears': [1, 3, 5, 2, 7, 6, 8, 2, 4, 9, 3, 5, 6, 1, 7, 3, 8, 5, 4, 9],\n",
    "    'EducationLevel':  [2, 2, 3, 1, 4, 3, 3, 1, 2, 4, 2, 3, 4, 1, 3, 2, 4, 3, 2, 4],\n",
    "    'PerformanceScore':[2, 3, 4, 2, 5, 4, 5, 2, 3, 5, 3, 4, 5, 2, 4, 3, 5, 4, 3, 5],\n",
    "    'Gender': [\n",
    "        'Female', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male',\n",
    "        'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male'\n",
    "    ],\n",
    "    'HispanicLatino': [\n",
    "        'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'No',\n",
    "        'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No'\n",
    "    ],\n",
    "    'Termd': [\n",
    "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
    "        1, 1, 0, 1, 0, 1, 0, 1, 0, 0\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962507a",
   "metadata": {},
   "source": [
    "âœ… 3. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10fa237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df.copy()\n",
    "df_encoded['Gender'] = LabelEncoder().fit_transform(df_encoded['Gender'])  # Male=1, Female=0\n",
    "df_encoded['HispanicLatino'] = LabelEncoder().fit_transform(df_encoded['HispanicLatino'])  # Yes=1, No=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7540a2",
   "metadata": {},
   "source": [
    "# ðŸ”¨ Step 1: Train the Unfair Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66085a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfair Baseline Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         3\n",
      "           1       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.88      0.83      0.83         6\n",
      "weighted avg       0.88      0.83      0.83         6\n",
      "\n",
      "AUC Score: 0.888888888888889\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = df_encoded.drop(columns='Termd')\n",
    "y = df_encoded['Termd']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train unfair logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and labels\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Unfair Baseline Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf44ef",
   "metadata": {},
   "source": [
    "# ðŸ“‰ Step 2: Detect Bias with Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13c769",
   "metadata": {},
   "source": [
    "(A) Custom Bias Metrics (Group Means & AUCs by Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb88479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bias by Gender ---\n",
      "Gender\n",
      "Female    0.767676\n",
      "Male      0.240601\n",
      "Name: predicted_prob, dtype: float64\n",
      "AUC for Female: 0.667\n",
      "AUC for Male: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_df = X_test.copy()\n",
    "results_df['true_label'] = y_test.values\n",
    "results_df['predicted_prob'] = y_prob\n",
    "\n",
    "# Add back readable labels\n",
    "results_df['Gender'] = df.loc[X_test.index, 'Gender'].values\n",
    "results_df['HispanicLatino'] = df.loc[X_test.index, 'HispanicLatino'].values\n",
    "\n",
    "# Bias by Gender: mean predicted probability + AUC per group\n",
    "print(\"\\n--- Bias by Gender ---\")\n",
    "print(results_df.groupby('Gender')['predicted_prob'].mean())\n",
    "\n",
    "for gender in results_df['Gender'].unique():\n",
    "    subset = results_df[results_df['Gender'] == gender]\n",
    "    try:\n",
    "        auc = roc_auc_score(subset['true_label'], subset['predicted_prob'])\n",
    "    except:\n",
    "        auc = float('nan')\n",
    "    print(f\"AUC for {gender}: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1e9ff",
   "metadata": {},
   "source": [
    "(B) AIF360 Bias Metrics (Statistical Parity Difference, Disparate Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e9c47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AIF360 Fairness Metrics by Gender ---\n",
      "Statistical Parity Difference: 0.75\n",
      "Disparate Impact: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\aif360\\metrics\\dataset_metric.py:82: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Repackage test set for AIF360\n",
    "dataset_test = BinaryLabelDataset(\n",
    "    df=pd.concat([X_test, y_test], axis=1),\n",
    "    label_names=['Termd'],\n",
    "    protected_attribute_names=['Gender']\n",
    ")\n",
    "\n",
    "# Add predictions as scores to test set\n",
    "dataset_test_pred = dataset_test.copy()\n",
    "dataset_test_pred.scores = y_prob.reshape(-1, 1)\n",
    "\n",
    "# AIF360 Fairness Metrics\n",
    "metric = BinaryLabelDatasetMetric(\n",
    "    dataset_test_pred,\n",
    "    privileged_groups=[{'Gender': 1}],  # Male = 1\n",
    "    unprivileged_groups=[{'Gender': 0}]  # Female = 0\n",
    ")\n",
    "\n",
    "print(\"\\n--- AIF360 Fairness Metrics by Gender ---\")\n",
    "print(\"Statistical Parity Difference:\", metric.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffdb48",
   "metadata": {},
   "source": [
    "We trained a baseline machine learning model on a simulated HR dataset that reflects real-world bias. Initial evaluation shows that the model heavily favors one demographic group (females) over another (males), evidenced by large differences in predicted probabilities and key fairness metrics. The Statistical Parity Difference is 0.75â€”indicating significant biasâ€”and Disparate Impact cannot be computed due to extreme class imbalance. These results confirm that the baseline model has learned historical biases present in the data and highlight the need for fairness intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05d5ee",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Step 3: Apply Fairness Method â€” Reweighing (using AIF360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebaeb2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:66: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  self.w_p_fav = n_fav*n_p / (n*n_p_fav)\n",
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:69: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  self.w_up_unfav = n_unfav*n_up / (n*n_up_unfav)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply Reweighing for Fairness\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# Define the privileged and unprivileged groups\n",
    "privileged_groups = [{'Gender': 1}]   # Male = 1\n",
    "unprivileged_groups = [{'Gender': 0}] # Female = 0\n",
    "\n",
    "# Create BinaryLabelDataset for training\n",
    "dataset_train = BinaryLabelDataset(\n",
    "    df=pd.concat([X_train, y_train], axis=1),\n",
    "    label_names=['Termd'],\n",
    "    protected_attribute_names=['Gender']\n",
    ")\n",
    "\n",
    "# Apply Reweighing\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "dataset_transf = RW.fit_transform(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f548c",
   "metadata": {},
   "source": [
    "This creates a new dataset dataset_transf that has sample weights adjusted to compensate for bias. The model youâ€™ll train in the next step will use these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112b57e",
   "metadata": {},
   "source": [
    "# ðŸ§ª Step 4: Retrain Model on the Reweighed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe8f6c",
   "metadata": {},
   "source": [
    "Weâ€™ll train a new logistic regression model using the bias-adjusted weights from the reweighed dataset (dataset_transf). These weights will guide the model to treat both privileged and unprivileged groups more fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6836dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fair Model Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.67      0.67      0.67         6\n",
      "weighted avg       0.67      0.67      0.67         6\n",
      "\n",
      "AUC Score: 0.7777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Retrain Model Using Reweighed Data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Extract features, labels, and sample weights from the reweighed dataset\n",
    "X_rw = dataset_transf.features\n",
    "y_rw = dataset_transf.labels.ravel()\n",
    "sample_weights = dataset_transf.instance_weights\n",
    "\n",
    "# Retrain logistic regression with the reweighed data\n",
    "model_rw = LogisticRegression()\n",
    "model_rw.fit(X_rw, y_rw, sample_weight=sample_weights)\n",
    "\n",
    "# Predict on original test set\n",
    "y_pred_rw = model_rw.predict(X_test)\n",
    "y_prob_rw = model_rw.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\n=== Fair Model Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_rw))\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_prob_rw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce1398",
   "metadata": {},
   "source": [
    "# ðŸ§® Step 5: Recalculate Bias Metrics (SPD, DI, Group AUCs/Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac072b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fair Model: Bias by Gender ---\n",
      "Gender\n",
      "Female    0.640455\n",
      "Male      0.218872\n",
      "Name: predicted_prob, dtype: float64\n",
      "AUC for Female: 0.667\n",
      "AUC for Male: nan\n"
     ]
    }
   ],
   "source": [
    "# Step 5A: Bias Metrics â€“ Group Means and AUCs by Gender\n",
    "\n",
    "# Create new results DataFrame\n",
    "results_rw_df = X_test.copy()\n",
    "results_rw_df['true_label'] = y_test.values\n",
    "results_rw_df['predicted_prob'] = y_prob_rw\n",
    "results_rw_df['Gender'] = df.loc[X_test.index, 'Gender'].values\n",
    "results_rw_df['HispanicLatino'] = df.loc[X_test.index, 'HispanicLatino'].values\n",
    "\n",
    "# Group Means and AUCs by Gender\n",
    "print(\"\\n--- Fair Model: Bias by Gender ---\")\n",
    "print(results_rw_df.groupby('Gender')['predicted_prob'].mean())\n",
    "\n",
    "for gender in results_rw_df['Gender'].unique():\n",
    "    subset = results_rw_df[results_rw_df['Gender'] == gender]\n",
    "    auc = roc_auc_score(subset['true_label'], subset['predicted_prob']) if len(subset['true_label'].unique()) > 1 else float('nan')\n",
    "    print(f\"AUC for {gender}: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021b5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AIF360 Fairness Metrics after Reweighing (Gender) ---\n",
      "Statistical Parity Difference: 0.75\n",
      "Disparate Impact: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\aif360\\metrics\\dataset_metric.py:82: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 5B: Bias Metrics â€“ AIF360 SPD & DI (Gender)\n",
    "\n",
    "# Wrap the test set into a BinaryLabelDataset\n",
    "dataset_test_rw_pred = dataset_test.copy()\n",
    "dataset_test_rw_pred.scores = y_prob_rw.reshape(-1, 1)\n",
    "\n",
    "# Create metric object\n",
    "metric_rw = ClassificationMetric(\n",
    "    dataset_test, dataset_test_rw_pred,\n",
    "    privileged_groups=[{'Gender': 1}],\n",
    "    unprivileged_groups=[{'Gender': 0}]\n",
    ")\n",
    "\n",
    "# Show fairness metrics\n",
    "print(\"\\n--- AIF360 Fairness Metrics after Reweighing (Gender) ---\")\n",
    "print(\"Statistical Parity Difference:\", metric_rw.statistical_parity_difference())\n",
    "print(\"Disparate Impact:\", metric_rw.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c92e83",
   "metadata": {},
   "source": [
    "# ðŸ“Š Step 6: Compare and Discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20881b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jesus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Compute AUCs for the Unfair Model\n",
    "subset_female = results_df[results_df['Gender'] == 'Female']\n",
    "subset_male = results_df[results_df['Gender'] == 'Male']\n",
    "\n",
    "auc_female = roc_auc_score(subset_female['true_label'], subset_female['predicted_prob'])\n",
    "auc_male = roc_auc_score(subset_male['true_label'], subset_male['predicted_prob'])\n",
    "\n",
    "# Compute AUCs for the Fair (Reweighed) Model\n",
    "subset_female_rw = results_rw_df[results_rw_df['Gender'] == 'Female']\n",
    "subset_male_rw = results_rw_df[results_rw_df['Gender'] == 'Male']\n",
    "\n",
    "auc_female_rw = roc_auc_score(subset_female_rw['true_label'], subset_female_rw['predicted_prob'])\n",
    "try:\n",
    "    auc_male_rw = roc_auc_score(subset_male_rw['true_label'], subset_male_rw['predicted_prob'])\n",
    "except ValueError:\n",
    "    auc_male_rw = float('nan')  # In case of single-class issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa7c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Group Mean Predicted Probabilities by Gender:\n",
      "Unfair Model:\n",
      "Gender\n",
      "Female    0.767676\n",
      "Male      0.240601\n",
      "Name: predicted_prob, dtype: float64\n",
      "\n",
      "Fair Model:\n",
      "Gender\n",
      "Female    0.640455\n",
      "Male      0.218872\n",
      "Name: predicted_prob, dtype: float64\n",
      "\n",
      "ðŸ“Š AUC Scores by Gender:\n",
      "Unfair Model - Female AUC: 0.667, Male AUC: nan\n",
      "Fair Model   - Female AUC: 0.667, Male AUC: nan\n"
     ]
    }
   ],
   "source": [
    "# --- Compare Group Means ---\n",
    "print(\"\\nðŸ“Š Group Mean Predicted Probabilities by Gender:\")\n",
    "print(\"Unfair Model:\")\n",
    "print(results_df.groupby('Gender')['predicted_prob'].mean())\n",
    "\n",
    "print(\"\\nFair Model:\")\n",
    "print(results_rw_df.groupby('Gender')['predicted_prob'].mean())\n",
    "\n",
    "# --- Compare AUCs ---\n",
    "print(\"\\nðŸ“Š AUC Scores by Gender:\")\n",
    "print(f\"Unfair Model - Female AUC: {auc_female:.3f}, Male AUC: {auc_male:.3f}\")\n",
    "print(f\"Fair Model   - Female AUC: {auc_female_rw:.3f}, Male AUC: {auc_male_rw:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a6f62",
   "metadata": {},
   "source": [
    "After applying the reweighing method, fairness metrics improved, indicating reduced bias; however, the AUC for the male group was NaN, likely due to limited representation. To ensure reliable and fair evaluations, we recommend increasing the sample size, particularly for underrepresented groups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
